{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This is my 2nd project in the deep RL course offered by Udacity. [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ddpg_multiagent import Agent\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_itter = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_itter.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_itter.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets train the DDPG Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size,\n",
    "              action_size=action_size, random_seed=777,discount_rew=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 10\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 15\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 20\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 25\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 30\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 35\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 40\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 45\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 50\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 55\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 60\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 65\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 70\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 75\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 80\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.00\tEpisode score: -0.00\n",
      "Episode 85\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 90\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 95\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 100\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 105\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 110\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 115\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 120\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 125\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 130\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 135\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 140\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 145\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 150\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 155\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 160\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 165\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 170\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 175\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 180\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 185\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 190\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.09\tEpisode score: -0.00\n",
      "Episode 195\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: -0.00\n",
      "Episode 200\tAverage Score: 0.00\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.050\n",
      "Episode 205\tAverage Score: 0.01\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: -0.00\n",
      "Episode 210\tAverage Score: 0.01\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.050\n",
      "Episode 215\tAverage Score: 0.01\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: -0.00\n",
      "Episode 220\tAverage Score: 0.01\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.050\n",
      "Episode 225\tAverage Score: 0.02\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.05\n",
      "Episode 230\tAverage Score: 0.02\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.05\n",
      "Episode 235\tAverage Score: 0.03\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.05\n",
      "Episode 240\tAverage Score: 0.03\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.050\n",
      "Episode 245\tAverage Score: 0.04\tMin Score: 0.00\tMax Score: 0.10\tEpisode score: 0.05\n",
      "Episode 250\tAverage Score: 0.04\tMin Score: 0.00\tMax Score: 0.19\tEpisode score: 0.15\n",
      "Episode 255\tAverage Score: 0.05\tMin Score: 0.00\tMax Score: 0.20\tEpisode score: 0.05\n",
      "Episode 260\tAverage Score: 0.05\tMin Score: 0.00\tMax Score: 0.20\tEpisode score: 0.05\n",
      "Episode 265\tAverage Score: 0.06\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.10\n",
      "Episode 270\tAverage Score: 0.07\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 275\tAverage Score: 0.07\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 280\tAverage Score: 0.08\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 285\tAverage Score: 0.08\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 290\tAverage Score: 0.09\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 295\tAverage Score: 0.09\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 300\tAverage Score: 0.09\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 305\tAverage Score: 0.09\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.05\n",
      "Episode 310\tAverage Score: 0.10\tMin Score: 0.00\tMax Score: 0.40\tEpisode score: 0.15\n",
      "Episode 315\tAverage Score: 0.12\tMin Score: 0.00\tMax Score: 1.20\tEpisode score: 0.10\n",
      "Episode 320\tAverage Score: 0.12\tMin Score: 0.00\tMax Score: 1.20\tEpisode score: 0.10\n",
      "Episode 325\tAverage Score: 0.14\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 2.60\n",
      "Episode 330\tAverage Score: 0.21\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.100\n",
      "Episode 335\tAverage Score: 0.29\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 1.25\n",
      "Episode 340\tAverage Score: 0.29\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.050\n",
      "Episode 345\tAverage Score: 0.30\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.65\n",
      "Episode 350\tAverage Score: 0.31\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.05\n",
      "Episode 355\tAverage Score: 0.31\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.15\n",
      "Episode 360\tAverage Score: 0.31\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.05\n",
      "Episode 365\tAverage Score: 0.34\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 2.40\n",
      "Episode 370\tAverage Score: 0.35\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.05\n",
      "Episode 375\tAverage Score: 0.40\tMin Score: 0.00\tMax Score: 2.60\tEpisode score: 0.150\n",
      "Episode 380\tAverage Score: 0.53\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.65\n",
      "Episode 385\tAverage Score: 0.53\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.05\n",
      "Episode 390\tAverage Score: 0.53\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.15\n",
      "Episode 395\tAverage Score: 0.54\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.15\n",
      "Episode 400\tAverage Score: 0.54\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.15\n",
      "Episode 405\tAverage Score: 0.55\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.75\n",
      "Episode 410\tAverage Score: 0.56\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.05\n",
      "Episode 415\tAverage Score: 0.56\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.05\n",
      "Episode 420\tAverage Score: 0.56\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.05\n",
      "Episode 425\tAverage Score: 0.54\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.15\n",
      "Episode 430\tAverage Score: 0.52\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.10\n",
      "Episode 435\tAverage Score: 0.50\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.10\n",
      "Episode 440\tAverage Score: 0.56\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.45\n",
      "Episode 445\tAverage Score: 0.59\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.60\n",
      "Episode 450\tAverage Score: 0.66\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.60\n",
      "Episode 455\tAverage Score: 0.71\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.60\n",
      "Episode 460\tAverage Score: 0.72\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.050\n",
      "Episode 465\tAverage Score: 0.69\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.100\n",
      "Episode 470\tAverage Score: 0.71\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.20\n",
      "Episode 475\tAverage Score: 0.73\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.05\n",
      "Episode 480\tAverage Score: 0.63\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.20\n",
      "Episode 485\tAverage Score: 0.64\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 490\tAverage Score: 0.66\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.20\n",
      "Episode 495\tAverage Score: 0.71\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.95\n",
      "Episode 500\tAverage Score: 0.76\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.45\n",
      "Episode 505\tAverage Score: 0.80\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.70\n",
      "Episode 510\tAverage Score: 0.81\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.10\n",
      "Episode 515\tAverage Score: 0.87\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.40\n",
      "Episode 520\tAverage Score: 0.92\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.55\n",
      "Episode 525\tAverage Score: 0.94\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.75\n",
      "Episode 530\tAverage Score: 0.94\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.75\n",
      "Episode 535\tAverage Score: 0.94\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 0.85\n",
      "Episode 540\tAverage Score: 1.00\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.65\n",
      "Episode 545\tAverage Score: 1.08\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.65\n",
      "Episode 550\tAverage Score: 1.10\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.50\n",
      "Episode 555\tAverage Score: 1.15\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 1.75\n",
      "Episode 560\tAverage Score: 1.24\tMin Score: 0.00\tMax Score: 2.70\tEpisode score: 2.60\n",
      "Episode 565\tAverage Score: 1.31\tMin Score: 0.09\tMax Score: 2.70\tEpisode score: 1.45\n",
      "Episode 570\tAverage Score: 1.35\tMin Score: 0.09\tMax Score: 2.70\tEpisode score: 2.60\n",
      "Episode 575\tAverage Score: 1.37\tMin Score: 0.09\tMax Score: 2.70\tEpisode score: 1.70\n",
      "Episode 580\tAverage Score: 1.40\tMin Score: 0.10\tMax Score: 2.70\tEpisode score: 0.05\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=5000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #restart env \n",
    "        env_itter = env.reset(train_mode=True)[brain_name] #note training = True here\n",
    "        #grab state of agents\n",
    "        state =env_itter.vector_observations\n",
    "        agent.reset()\n",
    "        score = np.zeros(len(env_itter.agents)) \n",
    "        \n",
    "        for t in range(max_t):\n",
    "            #select an action for each agent based off learned weights\n",
    "            #put a 1 instead of i_episodes if you do not want to decay the noise param\n",
    "            action = agent.act(state,i_episode,add_noise=True)\n",
    "            #send agents into env\n",
    "            env_itter = env.step(action)[brain_name]  \n",
    "            \n",
    "            #pull out env results\n",
    "            rewards = env_itter.rewards                      \n",
    "            dones = env_itter.local_done\n",
    "            next_states =env_itter.vector_observations\n",
    "            #train agents\n",
    "            agent.step(state, action, rewards, next_states, dones)\n",
    "            score += rewards      \n",
    "            state = next_states \n",
    "            #check if any agents finished\n",
    "            if np.any(dones):\n",
    "                break \n",
    "                \n",
    "        #note we now only append the max score over both agents to the final score tally\n",
    "        scores_deque.append(np.max(score))\n",
    "        scores.append(np.max(score))\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMin Score: {:.2f}\\tMax Score: {:.2f}\\tEpisode score: {:.2f}'.format(i_episode, np.mean(scores_deque),np.min(scores_deque),np.max(scores_deque),np.mean(score)), end=\"\")\n",
    "        if i_episode % 25 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "        \n",
    "        if np.mean(scores_deque) >= 0.5:\n",
    "            print('\\nDone! Solved task in {} Episodes'.format(i_episode))\n",
    "            torch.save(agent.actor_local.state_dict(), 'solved_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'solved_critic.pth')\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the episode average score\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), np.mean(scores,axis=1))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great! we solved the env in 143 episodes.\n",
    "\n",
    "The learning looks farily stable with this model arch and weighted noise approach.\n",
    "\n",
    "Below are results from other iterations to understand how stable the learning is over a longer time horizon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when we remove the noise scaler like the origional paper does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_noise_scale = pd.read_csv('no_noise_scale_scores.csv',index_col=0)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(np.array(no_noise_scale))+1), np.mean(np.array(no_noise_scale),axis=1))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that learning is very stable in for this change, but the agents faile to get an average score >= 30 (it capped out at just over 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I believe that further improvements can be made to the network architecture. For example, when I included batchnorm in between all layers of the network, the model struggled to learn at the same pace as the proposed solution. Further work on refining the architecture and network parameters will improve the results.\n",
    "\n",
    "Adding prioritized experience replay might improve the algorithms learning. By sampling the largest error observations with higher priority, the network will be able to maximize the information gain for each learning step.\n",
    "\n",
    "It would be interesting to see if an RNN architecture could improve the results. Capturing temporal dependencies among past actions could improve the agents ability to discern optimal actions at the current time step. Combining the actor network with a Bidirectional LSTM encoder/decoder architecture will be my next experiment!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
